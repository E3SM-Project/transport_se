Index: prim_advection_mod.F90
===================================================================
--- prim_advection_mod.F90	(revision 1965)
+++ prim_advection_mod.F90	(working copy)
@@ -15,37 +15,17 @@
       SEM 2D RK2 can use sign-preserving or monotone reconstruction
 
 Notes on Lagrange+REMAP advection
+dynamics looks like (i.e. for qsplit=3)
 
-From dynamics, we have the velocity on the reference levels which
-have density:  dp(t-1)    dp(t)      dp(t+1)
-
-Note that in a hydrostatic model, the density still satisifies the
-continuity equation exactly:  
-(1)      dp(t+1)-dp(t-1)  + 2dt div(U dp(t)) + 2dt d( eta_dot_dpdn(t) ) = 0 
-
-We introduce the vertically lagrangian levels, which have density
-              dp_star(t-1)   dp(t)   dp_star(t+1)
-We want dp_star(t) to be the density field which satisfies the 2D 
-continuity equation: 
-(2)       dp_star(t+1) - dp_star(t-1) + 2dt div(U dp(t) ) = 0
-
-Combining (1) and (2), we have:
-  dp_star(t+1) - dp_star(t-1) =  dp(t+1)-dp(t-1)  + 2dt d( eta_dot_dpdn(t) ) 
-
-Thus it is natural to define:
-(3)       dp_star(t+1) = dp(t+1) + dt d( eta_dot_dpdn(t) ) 
-(4)       dp_star(t-1) = dp(t-1) - dt d( eta_dot_dpdn(t) ) 
-
-If we use a forward-in-time advection scheme:
-    dp(t+1)-dp(t)   = dt div(Udp1) + dt d(eta_dot_dpdn1) = 0 
-    dp(t+2)-dp(t+1) = dt div(Udp2) + dt d(eta_dot_dpdn2) = 0 
-    dp(t+3)-dp(t+2) = dt div(Udp3) + dt d(eta_dot_dpdn3) = 0 
+    dp(t+1)-dp(t)   = -dt div(Udp1) - dt d(eta_dot_dpdn1)  + D(dp_diff1)
+    dp(t+2)-dp(t+1) = -dt div(Udp2) - dt d(eta_dot_dpdn2)  + D(dp_diff2)
+    dp(t+3)-dp(t+2) = -dt div(Udp3) - dt d(eta_dot_dpdn3)  + D(dp_diff3)
     ---------------
-    dp(t+3)-dp(t)   = 3dt div(Udp_sum/3) + 3dt d(eta_dot_dpdn_sum/3) = 0 
+    dp(t+3)-dp(t)        = -3dt div(Udp_sum/3) - 3dt d(eta_dot_dpdn_sum/3)  + D(dp_diff_ave)
+    dpstart(t+3) - dp(t) = -3dt div(Udp_sum/3)  + D(dp_diff_ave)
+OR:
+    dp_star(t+3) = dp(t+1) + 3dt d( eta_dot_dpdn_ave(t) ) 
 
-so we can define
-    dp_star(t+1) = dp(t+1) + 3dt d( eta_dot_dpdn_ave(t) ) 
-
 For RK2 advection of Q:
 For consistency, if Q=1
   dp1  = dp(t)- dt div[ U1 dp(t)]        U1 = U(t)
@@ -70,15 +50,7 @@
 so that
   Qdp(t+1) = Qdp(t) + dt/2 div[  Udp_ave/dp(t) Qdp(t)  + Udp_ave/dp1 Qdp1 ]
 
-Until we switch to forward-in-time scheme for dynamics, what should
-we use for U2?  
 
-(A) remapping U(t+1) from dp(t+1) to dp_star(t+1) is expensive.  
-
-(B) Could use (similar Eq 3 above) U2 = U(t+1) + dt*v_vadv(t)
-v_vadv can be computed from preq_vadv() subroutine with the
-DSSd eta_dot_dpdn.  No need to DSS v_vadv.  
-
 #endif
 
 
@@ -352,7 +324,6 @@
   subroutine remap_velocityQ(n0,np1,dt,elem,hvcoord,nets,nete,compute_diagnostics,rkstage)
   
     use physical_constants, only : cp, cpwater_vapor
-    use control_mod, only        : compute_mean_flux, prescribed_wind
 	
     implicit none
     real (kind=real_kind),  intent(in)        :: dt
@@ -361,7 +332,6 @@
     logical,        intent(in)              :: compute_diagnostics
     
     integer :: nets,nete,n0,np1,rkstage
-    logical :: use_mean_flux=.false.
     
     ! ========================
     ! Local Variables
@@ -378,8 +348,6 @@
     
     call t_startf('remap_velocityQ')
 
-    if(compute_mean_flux==1 .and. prescribed_wind==0) use_mean_flux=.true.
-   
     do ie=nets,nete
 #if (defined ELEMENT_OPENMP)
 !$omp parallel do private(q,i,j,z1c,z2c,zv,k,dp_np1,dp_star,Qcol,zkr,ilev) &
@@ -394,24 +362,16 @@
              z2c(1)=0
              zv(1)=0
              do k=1,nlev
-                if(use_mean_flux)then
-                   dp_np1 = (hvcoord%hyai(k+1) - hvcoord%hyai(k))*hvcoord%ps0 + &
-                        (hvcoord%hybi(k+1) - hvcoord%hybi(k))*elem(ie)%state%ps_v(i,j,np1)
+                dp_np1 = (hvcoord%hyai(k+1) - hvcoord%hyai(k))*hvcoord%ps0 + &
+                     (hvcoord%hybi(k+1) - hvcoord%hybi(k))*elem(ie)%state%ps_v(i,j,np1)
+                dp_star = dp_np1 + dt*(elem(ie)%derived%eta_dot_dpdn(i,j,k+1) & 
+                     -elem(ie)%derived%eta_dot_dpdn(i,j,k)) 
 #ifdef ZEROVERT                        
-                   dp_star = dp_np1  ! ignore the vertical motion
-#else
-                   dp_star = elem(ie)%derived%dp(i,j,k)-dt*elem(ie)%derived%divdp_proj(i,j,k)
-#endif        
-                else
-                   dp_np1 = (hvcoord%hyai(k+1) - hvcoord%hyai(k))*hvcoord%ps0 + &
-                        (hvcoord%hybi(k+1) - hvcoord%hybi(k))*elem(ie)%state%ps_v(i,j,np1)
-#ifdef ZEROVERT                        
-                   dp_star = dp_np1  ! ignore the vertical motion
-#else
-                   dp_star = dp_np1 + dt*(elem(ie)%derived%eta_dot_dpdn(i,j,k+1) & 
-                        -elem(ie)%derived%eta_dot_dpdn(i,j,k)) 
+                ! ignore the vertical motion
+                dp_star = (hvcoord%hyai(k+1) - hvcoord%hyai(k))*hvcoord%ps0 + &
+                     (hvcoord%hybi(k+1) - hvcoord%hybi(k))*elem(ie)%state%ps_v(i,j,np1)
 #endif
-                endif
+
                 z1c(k+1) = z1c(k)+dp_star
                 z2c(k+1) = z2c(k)+dp_np1
 #ifdef ZEROHORZ			  
@@ -650,7 +610,6 @@
   subroutine remap_velocityC(n0,np1,dt,elem,cslam,hvcoord,nets,nete,compute_diagnostics)
   
     use physical_constants, only : cp, cpwater_vapor
-    use control_mod, only        : compute_mean_flux, prescribed_wind
 	
     implicit none
     real (kind=real_kind),  intent(in)        :: dt
@@ -660,7 +619,6 @@
     logical,        intent(in)              :: compute_diagnostics
     
     integer :: nets,nete,n0,np1
-    logical :: use_mean_flux=.false.
     
     ! ========================
     ! Local Variables
@@ -1012,12 +970,18 @@
   subroutine Prim_Advec_Init()
     use dimensions_mod, only : nlev, qsize, nelemd
 
-    call initEdgeBuffer(edgeAdv,qsize*nlev)
     call initEdgeBuffer(edgeAdv1,nlev)
+    call initEdgeBuffer(edgeAdv,qsize*nlev)
     call initEdgeBuffer(edgeAdv_p1,qsize*nlev + nlev) 
     call initEdgeBuffer(edgeAdvQ2,qsize*nlev*2)  ! Qtens,Qmin, Qmax
     call initEdgeBuffer(edgeAdvQ3,qsize*nlev*3)  ! Qtens,Qmin, Qmax
 
+    call replace_edge_buffers(edgeAdvQ2,edgeAdvQ2%nlyr,edgeAdvQ2%nbuf,edgeAdvQ3%buf,edgeAdvQ3%receive)
+    call replace_edge_buffers(edgeAdv_p1,edgeAdv_p1%nlyr,edgeAdv_p1%nbuf,edgeAdvQ3%buf,edgeAdvQ3%receive)
+    call replace_edge_buffers(edgeAdv1,edgeAdv1%nlyr,edgeAdv1%nbuf,edgeAdvQ3%buf,edgeAdvQ3%receive)
+    call replace_edge_buffers(edgeAdv,edgeAdv%nlyr,edgeAdv%nbuf,edgeAdvQ3%buf,edgeAdvQ3%receive)
+
+
     ! this static array is shared by all threads, so dimension for all threads (nelemd), not nets:nete:
     allocate (qmin(nlev,qsize,nelemd))
     allocate (qmax(nlev,qsize,nelemd))
@@ -1090,7 +1054,6 @@
     ! 2D advection step
     ! 
     !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
-    ! elem%derived%vn0   = velocity at time t on reference levels (saved from dynamics)
     ! elem%state%u(np1)  = velocity at time t+1 on reference levels
     ! elem%derived%vstar = velocity at t+1 on floating levels (computed below)
     call remap_velocityUV(np1,dt,elem,hvcoord,nets,nete)
@@ -1203,10 +1166,9 @@
     ! note: stage 3 we take the oppertunity to DSS omega
     !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
     if(compute_mean_flux==1)then
-      ! mean flux and dp(n0) was computed by dynamics
       ! use these for consistent advection (preserve Q=1)
-      ! derived%vn0             =  mean horiz. flux:   U*dp
-      ! derived%dp              =  dp at n0.  needed for remap.  
+      ! derived%vdp_ave        =  mean horiz. flux:   U*dp
+      ! derived%eta_dot_dpdn    =  mean vertical velocity (used for remap)
       ! derived%omega_p         =  advection code will DSS this for the physics, but otherwise 
       !                            it is not needed 
       ! Also: save a copy of div(U dp) in derived%div(:,:,:,1), which will be DSS'd 
@@ -1232,11 +1194,11 @@
       
       rhs_multiplier = 1
       call euler_step(np1,np1,dt/2,elem,hvcoord,hybrid,deriv,nets,nete,&
-           .false.,USEconsistent,DSSomega,rhs_multiplier)
+           .false.,USEconsistent,DSSeta,rhs_multiplier)
       
       rhs_multiplier = 2
       call euler_step(np1,np1,dt/2,elem,hvcoord,hybrid,deriv,nets,nete,&
-           .false.,USEconsistent,DSSno_var,rhs_multiplier)
+           .false.,USEconsistent,DSSomega,rhs_multiplier)
       
     else
       ! non-consistent code uses (computed in the dynamics)
@@ -1680,20 +1642,7 @@
   !   time-split
   !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
   if (limiter_option == 8) then
-     do ie=nets,nete
-        ! add hyperviscosity to RHS.  apply to Q at timelevel n0, Qdp(n0)/dp
-#if (defined ELEMENT_OPENMP)
-!$omp parallel do private(k, q)
-#endif
-        do k=1,nlev    !  Loop index added with implicit inversion (AAM)
-           dp(:,:,k) = elem(ie)%derived%dp(:,:,k) - &
-                rhs_multiplier*dt*elem(ie)%derived%divdp_proj(:,:,k) 
-           do q=1,qsize
-              Qtens_biharmonic(:,:,k,q,ie) = elem(ie)%state%Qdp(:,:,k,q,n0)/dp(:,:,k)
-           enddo
-        enddo
-     enddo
-
+     !
      ! when running lim8, we also need to limit the biharmonic, so that term needs
      ! to be included in each euler step.  three possible algorithms here:
      ! 1) most expensive:
@@ -1714,12 +1663,47 @@
      !     compute qmin/qmax directly on first stage
      !     reuse qmin/qmax for 2nd stage stage (but update based on local qmin/qmax)
      !     cost:  1 biharmonic steps, 2 DSS
+     !
+     !  NOTE  when nu_p=0 (no dissipation applied in dynamics to dp equation), we should
+     !        apply dissipation to Q (not Qdp) to preserve Q=1
+     !        i.e.  laplace(Qdp) ~  dp_const laplace(Q)                
+     !        for nu_p=nu_q>0, we need to apply dissipation to Q * diffusion_dp
+
+
+     ! initialize dp, and compute Q from Qdp
+     ! for convience, we store Q in Qtens_biharmonic()
+     do ie=nets,nete
+        ! add hyperviscosity to RHS.  apply to Q at timelevel n0, Qdp(n0)/dp
+#if (defined ELEMENT_OPENMP)
+!$omp parallel do private(k, q)
+#endif
+        do k=1,nlev    !  Loop index added with implicit inversion (AAM)
+           dp(:,:,k) = elem(ie)%derived%dp(:,:,k) - &
+                rhs_multiplier*dt*elem(ie)%derived%divdp_proj(:,:,k) 
+           do q=1,qsize
+              Qtens_biharmonic(:,:,k,q,ie) = elem(ie)%state%Qdp(:,:,k,q,n0)/dp(:,:,k)
+           enddo
+        enddo
+     enddo
+
      if (rhs_multiplier == 0) then
-        call neighbor_minmax(elem,qtens_biharmonic,hybrid,edgeAdvQ2,nets,nete,qmin(:,:,nets:nete),qmax(:,:,nets:nete))
+        ! compute element qmin/qmax
+        do ie=nets,nete
+           do k=1,nlev    
+              do q=1,qsize
+                 qmin(k,q,ie)=minval(Qtens_biharmonic(:,:,k,q,ie))
+                 qmax(k,q,ie)=maxval(Qtens_biharmonic(:,:,k,q,ie))
+                 qmin(k,q,ie)=max(qmin(k,q,ie),0d0)
+              enddo
+           enddo
+        enddo
+        ! update qmin/qmax based on neighbor data
+        call neighbor_minmax(elem,hybrid,edgeAdvQ2,nets,nete,qmin(:,:,nets:nete),qmax(:,:,nets:nete))
         rhs_viss=0
      endif
+
      if (rhs_multiplier == 1 ) then
-        ! lets just reuse the neighbor min/max, but update based on local data
+        ! lets just reuse the old neighbor min/max, but update based on local data
         rhs_viss=0
         do ie=nets,nete
            do k=1,nlev    !  Loop index added with implicit inversion (AAM)
@@ -1731,8 +1715,22 @@
            enddo
         enddo
      endif
+
      if (rhs_multiplier == 2) then
         rhs_viss=3
+        ! compute element qmin/qmax  
+        do ie=nets,nete
+           do k=1,nlev    
+              do q=1,qsize
+                 qmin(k,q,ie)=minval(Qtens_biharmonic(:,:,k,q,ie))
+                 qmax(k,q,ie)=maxval(Qtens_biharmonic(:,:,k,q,ie))
+                 qmin(k,q,ie)=max(qmin(k,q,ie),0d0)
+              enddo
+           enddo
+        enddo
+        ! compute biharmonic operator, and during the DSS also update qmin/qmax based on neighbor data
+        ! qtens_biharmonic *= dp0
+        ! or:  qtens_biharmonc *= elem()psdiss_ave
         call biharmonic_wk_scalar_minmax(elem,qtens_biharmonic,deriv,edgeAdvQ3,hybrid,&
              nets,nete,qmin(:,:,nets:nete),qmax(:,:,nets:nete))
 
@@ -1769,18 +1767,7 @@
      if ( DSSopt == DSSdiv_vdp_ave) DSSvar => elem(ie)%derived%divdp_proj(:,:,:)
 
 
-     ! dp for RK stage initial time 
-#if (defined ELEMENT_OPENMP)
-!$omp parallel do private(k)
-#endif
-     do k=1,nlev    !  Loop index added (AAM)
-        dp(:,:,k) = elem(ie)%derived%dp(:,:,k) - &
-          rhs_multiplier*dt*elem(ie)%derived%divdp_proj(:,:,k) 
-     enddo
 
-     ! un-DSS'ed dp for RK stage end time
-     ! dp_star = dp - dt*elem(ie)%derived%divdp(:,:,:)   computed below, if needed 
-
      ! Compute velocity used to advance Qdp 
      if (Uopt==USEv) then
 #if (defined ELEMENT_OPENMP)
@@ -1810,34 +1797,23 @@
       do k=1,nlev    !  Loop index added (AAM)
        Vstar(:,:,:,k) = (elem(ie)%derived%vstar(:,:,:,k) + elem(ie)%derived%vn0(:,:,:,k))/2
       enddo
-     else if (Uopt==USEconsistent) then
-
-!if consistent subcycling then vn0 = UR, mean flux
-!UR is defined in prim_advance_exp so that eqn for density is
-!dp(n+1)=dp(n)-qsplit*dt_dynamics*DIV(UR), n is dynamics timestep
-!for example, for qsplit=4 UR=((udp)_2+(udp)_4), _2 and _4 are 2nd and
-!4th stages in 1 dynamics timestep
-
-!also, Vstar is velocity for physics defined so that consistency holds.
-!it turns out that Vstar at each physics stage should be then UR/(dp_tracers)
-!therefore, dp_tracers(stage+1)=dp(stage)-dt_physics * DIV(UR)
-
-!note that UR comes unprojected ie it requires a DSS, so it is done
-!during the first stage in physics, because dp_tracers(stage=1)=dp(n)
-!and rhs_multiplier=0
-
+   else if (Uopt==USEconsistent) then
 #if (defined ELEMENT_OPENMP)
 !$omp parallel do private(k)
 #endif
       do k=1,nlev    !  Loop index added (AAM)
-       Vstar(:,:,1,k) = elem(ie)%derived%vn0(:,:,1,k)/dp(:,:,k)
-       Vstar(:,:,2,k) = elem(ie)%derived%vn0(:,:,2,k)/dp(:,:,k)
+         ! derived variable divdp_proj() (DSS'd version of divdp) will only be correct on 2nd and 3rd stage
+         ! but that's ok because rhs_multiplier=0 on the first stage:
+         dp(:,:,k) = elem(ie)%derived%dp(:,:,k) - &
+              rhs_multiplier*dt*elem(ie)%derived%divdp_proj(:,:,k) 
+         Vstar(:,:,1,k) = elem(ie)%derived%vn0(:,:,1,k)/dp(:,:,k)
+         Vstar(:,:,2,k) = elem(ie)%derived%vn0(:,:,2,k)/dp(:,:,k)
       enddo
-     else
-       stop 'ERROR:  bad Uopt'
-     endif
+   else
+      stop 'ERROR:  bad Uopt'
+   endif
+     
 
-
      ! advance Qdp
 #if (defined ELEMENT_OPENMP)
 !$omp parallel do private(q,k,gradQ,dp_star,qtens)
@@ -1870,6 +1846,7 @@
            do k=1,nlev  ! Loop index added (AAM)
               ! UN-DSS'ed dp at timelevel n0+1:  
               dp_star(:,:,k) = dp(:,:,k) - dt*elem(ie)%derived%divdp(:,:,k)  
+!             if (rhs_viss/=0) dp_star(:,:,k) = dp_star(:,:,k) + dt*elem(ie)%derived%psdiss_tend(:,:,k)  
            enddo
            ! apply limiter to Q = Qtens / dp_star 
 	   call limiter_optim_iter_full(Qtens(:,:,:),elem(ie)%spheremp(:,:),&
@@ -2971,10 +2948,32 @@
 
 
 
+!
+! subroutine to force all the tracer edge buffers to share the largest buffer
+! this has to be outside a module to allow us to (F77 style) access the same chunk 
+! of memory with a different shape
+!
+subroutine replace_edge_buffers(edge,nlyr,nbuf,newbuf,newreceive)
+use kinds, only          : real_kind,int_kind
+use edge_mod, only       : EdgeBuffer_t
+! input
+type (EdgeBuffer_t) :: edge
+integer :: nlyr,nbuf
+real(kind=real_kind) , target :: newbuf(nlyr,nbuf), newreceive(nlyr,nbuf)
 
+deallocate(edge%buf)
+deallocate(edge%receive)
+edge%buf => newbuf
+edge%receive => newreceive
+end subroutine replace_edge_buffers
 
 
 
 
 
+
+
+
+
+
  
