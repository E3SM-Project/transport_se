Index: derivative_mod.F90
===================================================================
--- derivative_mod.F90	(revision 2389)
+++ derivative_mod.F90	(revision 2419)
@@ -2206,8 +2206,8 @@
 ! hypervis_power =0 for const hv (1),  <>0 for variable hv (2) and tensor hv (3)
 ! if using (2), it is required to set also fine_ne, max_hypervis_courant
 
-#undef TENSORHV
-!#define TENSORHV
+!#undef TENSORHV
+#define TENSORHV
 
 
   function laplace_sphere_wk(s,deriv,elem,viscosity) result(laplace)
Index: cuda_mod.F90
===================================================================
--- cuda_mod.F90	(revision 2389)
+++ cuda_mod.F90	(revision 2419)
@@ -21,12 +21,14 @@
 
 module cuda_mod
 #ifdef _ACCEL
+#define PAD 1
+
 !Put everything CUDA-specific in here so it doesn't get compiled without -Mcuda enabled on a PGI compiler
   use cudafor
-  use kinds          , only : real_kind
-  use dimensions_mod, only: np,nlevp,nlev,qsize,qsize_d,max_corner_elem,max_neigh_edges,nelemd
-  use element_mod, only: timelevels
-  use edge_mod, only: EdgeBuffer_t
+  use kinds          , only: real_kind
+  use dimensions_mod , only: np,nlevp,nlev,qsize,qsize_d,max_corner_elem,max_neigh_edges,nelemd
+  use element_mod    , only: timelevels
+  use edge_mod       , only: EdgeBuffer_t
   implicit none
   private
 
@@ -34,6 +36,7 @@
   public :: cuda_mod_init
   public :: euler_step_cuda 
 
+  !This is from prim_advection_mod.F90
   type(EdgeBuffer_t) :: edgeAdv, edgeAdvQ3, edgeAdvQ2, edgeAdvDSS
   real(kind=real_kind), allocatable :: qmin(:,:,:), qmax(:,:,:)
   integer,parameter :: DSSeta = 1
@@ -41,8 +44,65 @@
   integer,parameter :: DSSdiv_vdp_ave = 3
   integer,parameter :: DSSno_var = -1
 
+  !Device arrays
+  real (kind=real_kind),device,allocatable,dimension(:,:,:,:,:,:) :: qdp_d
+  real (kind=real_kind),device,allocatable,dimension(:,:,:,:,:)   :: qtens_d
+  real (kind=real_kind),device,allocatable,dimension(:,:,:)       :: spheremp_d
+  real (kind=real_kind),device,allocatable,dimension(:,:,:)       :: rspheremp_d
+  real (kind=real_kind),device,allocatable,dimension(:,:,:,:,:)   :: dinv_d
+  real (kind=real_kind),device,allocatable,dimension(:,:,:)       :: variable_hyperviscosity_d
+  real (kind=real_kind),device,allocatable,dimension(:,:,:)       :: metdet_d
+  real (kind=real_kind),device,allocatable,dimension(:,:,:)       :: rmetdet_d
+  real (kind=real_kind),device,allocatable,dimension(:,:)         :: edgebuf_d
+  logical              ,device,allocatable,dimension(:,:)         :: reverse_d
+  integer              ,device,allocatable,dimension(:,:)         :: putmapP_d
+  integer              ,device,allocatable,dimension(:,:)         :: getmapP_d
+  real (kind=real_kind),device,allocatable,dimension(:,:,:,:,:)   :: vstar_d
+  real (kind=real_kind),device,allocatable,dimension(:,:,:,:)     :: dp_d
+  integer              ,device,allocatable,dimension(:)           :: send_nelem_d
+  integer              ,device,allocatable,dimension(:)           :: recv_nelem_d
+  integer              ,device,allocatable,dimension(:,:)         :: send_indices_d
+  integer              ,device,allocatable,dimension(:,:)         :: recv_indices_d
+  integer              ,device,allocatable,dimension(:)           :: recv_internal_indices_d
+  integer              ,device,allocatable,dimension(:)           :: recv_external_indices_d
+  real (kind=real_kind),device,allocatable,dimension(:,:,:)       :: recvbuf_d
 
+  !PINNED Host arrays
+  real(kind=real_kind),pinned,allocatable,dimension(:,:,:,:,:) :: Vstar_h
+  real(kind=real_kind),pinned,allocatable,dimension(:,:,:,:) :: dp_h
+  real(kind=real_kind),pinned,allocatable,dimension(:,:,:,:) :: dp_star_h
+  real(kind=real_kind),pinned,allocatable,dimension(:,:,:,:) :: dp_np1_h
+  real(kind=real_kind),pinned,allocatable,dimension(:,:,:) :: sendbuf_h
+  real(kind=real_kind),pinned,allocatable,dimension(:,:,:) :: recvbuf_h
 
+  !Normal Host arrays
+  integer,allocatable,dimension(:)   :: send_nelem
+  integer,allocatable,dimension(:)   :: recv_nelem
+  integer,allocatable,dimension(:,:) :: send_indices
+  integer,allocatable,dimension(:,:) :: recv_indices
+  integer,allocatable,dimension(:)   :: recv_internal_indices
+  integer,allocatable,dimension(:)   :: recv_external_indices
+  integer :: recv_external_nelem
+  integer :: recv_internal_nelem
+  logical :: old_peu
+  logical,allocatable,dimension(:)   :: d2h_done
+  logical,allocatable,dimension(:)   :: msg_sent
+  logical,allocatable,dimension(:)   :: msg_rcvd
+  logical,allocatable,dimension(:)   :: h2d_done
+  integer, parameter :: south_px = 1
+  integer, parameter :: east_px  = 3
+  integer, parameter :: north_px = 4
+  integer, parameter :: west_px  = 2
+  integer, parameter :: cuda_streams = 16
+  integer            :: streams(0:cuda_streams)
+  integer            :: streams2(0:cuda_streams)
+  integer            :: nbuf
+  integer            :: nmsg_rcvd
+  integer            :: nmsg_sent
+
+  type(cudaEvent) :: timer1, timer2
+
+
 contains
 
 
@@ -50,40 +110,256 @@
   !The point of this is to initialize any data required in other routines of this module as well
   !as to run one initial CUDA kernel just to get those overheads out of the way so that subsequent
   !timing routines are accurage.
-  subroutine cuda_mod_init()
+  subroutine cuda_mod_init(elem)
     use edge_mod      , only: initEdgeBuffer
+    use schedule_mod  , only: schedule_t, cycle_t, schedule
+    use edge_mod      , only: Edgebuffer_t
+    use element_mod   , only: element_t
     implicit none
-    integer     :: ierr, ie
-    type (dim3) :: griddim,blockdim
+    type(element_t), intent(in) :: elem(:)
+
+    type (Cycle_t),pointer    :: pCycle
+    type (Schedule_t),pointer :: pSchedule
+    integer                   :: ie , ierr , icycle , iPtr , rank , nSendCycles , nRecvCycles , nlyr , mx_send_len , mx_recv_len , n
+    real(kind=real_kind)      :: dinv_t(np , np , 2 , 2)
+    type (dim3)               :: griddim , blockdim
+    logical,allocatable,dimension(:,:) :: send_elem_mask
+    logical,allocatable,dimension(:,:) :: recv_elem_mask
+    logical,allocatable,dimension(:)   :: elem_computed
+    integer :: total_work
+
+#if (defined ELEMENT_OPENMP)
+    write(*,*) 'ERROR: Do not use ELEMENT_OPENMP and CUDA FORTRAN'
+    stop
+#endif
 !$OMP BARRIER
 !$OMP MASTER
     write(*,*) "cuda_mod_init"
 
+    write(*,*) "allocate arrays on device & host"
+#ifdef _PREDICT
+    pSchedule => Schedule(iam)
+#else
+    pSchedule => Schedule(1)
+#endif
+    nlyr = edgeAdv%nlyr
+    nSendCycles = pSchedule%nSendCycles
+    nRecvCycles = pSchedule%nRecvCycles
+    mx_send_len = 0
+    mx_recv_len = 0
+    do icycle=1,nSendCycles
+      if (pSchedule%SendCycle(icycle)%lengthP > mx_send_len) mx_send_len = pSchedule%SendCycle(icycle)%lengthP
+    enddo
+    do icycle=1,nRecvCycles
+      if (pSchedule%RecvCycle(icycle)%lengthP > mx_recv_len) mx_recv_len = pSchedule%RecvCycle(icycle)%lengthP 
+    enddo
+    nbuf=4*(np+max_corner_elem)*nelemd  !inlined from edge_mod.F90, initEdgeBuffer()
+
+    !Allocate the host and device arrays
+    allocate( qmin                     (nlev,qsize_d                 ,nelemd) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
+    allocate( qmax                     (nlev,qsize_d                 ,nelemd) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
+    allocate( qdp_d                    (np,np,nlev,qsize_d,timelevels,nelemd) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
+    allocate( qtens_d                  (np,np,nlev,qsize_d           ,nelemd) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
+    allocate( spheremp_d               (np,np                        ,nelemd) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
+    allocate( rspheremp_d              (np,np                        ,nelemd) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
+    allocate( dinv_d                   (np,np,2,2                    ,nelemd) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
+    allocate( variable_hyperviscosity_d(np,np                        ,nelemd) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
+    allocate( metdet_d                 (np,np                        ,nelemd) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
+    allocate( rmetdet_d                (np,np                        ,nelemd) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
+    allocate( vstar_d                  (np,np,nlev,2                 ,nelemd) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
+    allocate( dp_d                     (np,np,nlev                   ,nelemd) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
+    allocate( reverse_d                (max_neigh_edges              ,nelemd) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
+    allocate( putmapP_d                (max_neigh_edges              ,nelemd) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
+    allocate( getmapP_d                (max_neigh_edges              ,nelemd) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
+    allocate( recvbuf_d                (nlev*qsize_d,mx_recv_len,nRecvCycles) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
+    allocate( edgebuf_d                (nlev*qsize_d,nbuf                   ) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
+    allocate( send_nelem_d             (       nSendCycles                  ) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
+    allocate( recv_nelem_d             (       nRecvCycles                  ) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
+    allocate( send_indices_d           (nelemd,nSendCycles                  ) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
+    allocate( recv_indices_d           (nelemd,nRecvCycles                  ) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
+    allocate( recv_internal_indices_d  (nelemd                              ) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
+    allocate( recv_external_indices_d  (nelemd                              ) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
+
+    allocate( Vstar_h                  (np,np,nlev,2                 ,nelemd) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
+    allocate( dp_h                     (np,np,nlev                   ,nelemd) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
+    allocate( dp_star_h                (np,np,nlev                   ,nelemd) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
+    allocate( dp_np1_h                 (np,np,nlev                   ,nelemd) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
+    allocate( sendbuf_h                (nlev*qsize_d,mx_send_len,nSendCycles) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
+    allocate( recvbuf_h                (nlev*qsize_d,mx_recv_len,nRecvCycles) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
+    allocate( send_elem_mask           (nelemd,nSendCycles                  ) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
+    allocate( recv_elem_mask           (nelemd,nRecvCycles                  ) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
+    allocate( send_nelem               (       nSendCycles                  ) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
+    allocate( recv_nelem               (       nRecvCycles                  ) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
+    allocate( send_indices             (nelemd,nSendCycles                  ) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
+    allocate( recv_indices             (nelemd,nRecvCycles                  ) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
+    allocate( recv_internal_indices    (nelemd                              ) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
+    allocate( recv_external_indices    (nelemd                              ) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
+    allocate( send_elem_mask           (nelemd,nSendCycles                  ) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
+    allocate( recv_elem_mask           (nelemd,nRecvCycles                  ) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
+    allocate( elem_computed            (nelemd                              ) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
+    allocate( d2h_done                 (nSendCycles                         ) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
+    allocate( msg_sent                 (nSendCycles                         ) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
+    allocate( msg_rcvd                 (nRecvCycles                         ) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
+    allocate( h2d_done                 (nRecvCycles                         ) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
+
+    write(*,*) "send data from host to device"
+    !Copy over data to the device
+    do ie = 1,nelemd
+      dinv_t(:,:,1,1) = elem(ie)%dinv(1,1,:,:)
+      dinv_t(:,:,1,2) = elem(ie)%dinv(1,2,:,:)
+      dinv_t(:,:,2,1) = elem(ie)%dinv(2,1,:,:)
+      dinv_t(:,:,2,2) = elem(ie)%dinv(2,2,:,:)
+      ierr = cudaMemcpy( qdp_d                    (1,1,1,1,1,ie) , elem(ie)%state%Qdp               , size(elem(ie)%state%Qdp              ) , cudaMemcpyHostToDevice ) ; if ( ierr .ne. 0 ) stop __LINE__
+      ierr = cudaMemcpy( spheremp_d               (1,1      ,ie) , elem(ie)%spheremp                , size(elem(ie)%spheremp               ) , cudaMemcpyHostToDevice ) ; if ( ierr .ne. 0 ) stop __LINE__
+      ierr = cudaMemcpy( rspheremp_d              (1,1      ,ie) , elem(ie)%rspheremp               , size(elem(ie)%rspheremp              ) , cudaMemcpyHostToDevice ) ; if ( ierr .ne. 0 ) stop __LINE__
+      ierr = cudaMemcpy( dinv_d                   (1,1,1,1  ,ie) , dinv_t                           , size(dinv_t                          ) , cudaMemcpyHostToDevice ) ; if ( ierr .ne. 0 ) stop __LINE__
+      ierr = cudaMemcpy( metdet_d                 (1,1      ,ie) , elem(ie)%metdet                  , size(elem(ie)%metdet                 ) , cudaMemcpyHostToDevice ) ; if ( ierr .ne. 0 ) stop __LINE__
+      ierr = cudaMemcpy( rmetdet_d                (1,1      ,ie) , elem(ie)%rmetdet                 , size(elem(ie)%rmetdet                ) , cudaMemcpyHostToDevice ) ; if ( ierr .ne. 0 ) stop __LINE__
+      ierr = cudaMemcpy( putmapP_d                (1        ,ie) , elem(ie)%desc%putmapP            , size(elem(ie)%desc%putmapP           ) , cudaMemcpyHostToDevice ) ; if ( ierr .ne. 0 ) stop __LINE__
+      ierr = cudaMemcpy( getmapP_d                (1        ,ie) , elem(ie)%desc%getmapP            , size(elem(ie)%desc%getmapP           ) , cudaMemcpyHostToDevice ) ; if ( ierr .ne. 0 ) stop __LINE__
+      ierr = cudaMemcpy( reverse_d                (1        ,ie) , elem(ie)%desc%reverse            , size(elem(ie)%desc%reverse           ) , cudaMemcpyHostToDevice ) ; if ( ierr .ne. 0 ) stop __LINE__
+      ierr = cudaMemcpy( variable_hyperviscosity_d(1,1      ,ie) , elem(ie)%variable_hyperviscosity , size(elem(ie)%variable_hyperviscosity) , cudaMemcpyHostToDevice ) ; if ( ierr .ne. 0 ) stop __LINE__
+    enddo
+
+    write(*,*) "edgebuffers"
+    !These have to be in a threaded region or they complain and die
+!$OMP END MASTER
+    call initEdgeBuffer(edgeAdv   ,qsize_d*nlev  )
+    call initEdgeBuffer(edgeAdvDSS,      nlev  )
+    call initEdgeBuffer(edgeAdvQ2 ,qsize_d*nlev*2)
+    call initEdgeBuffer(edgeAdvQ3 ,qsize_d*nlev*3)
+!$OMP MASTER
+
+    write(*,*) "initial kernel"
+    !This needs to run because we need accurate timing during out cuda profiler runs. Initial kernel runs incur overheads, so we do this here
     blockdim = dim3(1,1,1)
     griddim  = dim3(1,1,1)
     call warmup <<< griddim , blockdim >>> ( ie )
     ierr = cudaThreadSynchronize()
 
-!$OMP END MASTER
-    write(*,*) __LINE__
-    call initEdgeBuffer(edgeAdv   ,qsize*nlev  )
-    write(*,*) __LINE__
-    call initEdgeBuffer(edgeAdvDSS,      nlev  )
-    write(*,*) __LINE__
-    call initEdgeBuffer(edgeAdvQ2 ,qsize*nlev*2)
-    write(*,*) __LINE__
-    call initEdgeBuffer(edgeAdvQ3 ,qsize*nlev*3)
-    write(*,*) __LINE__
-!$OMP MASTER
+    do n = 0 , cuda_streams
+      ierr = cudaStreamCreate(streams(n))
+      ierr = cudaStreamCreate(streams2(n))
+    enddo
+    ierr = cudaDeviceSetCacheConfig(cudaFuncCachePreferShared)
+    ierr = cudaEventCreate(timer1)
+    ierr = cudaEventCreate(timer2)
 
-    allocate(qmin(nlev,qsize,nelemd))
-    allocate(qmax(nlev,qsize,nelemd))
+    write(*,*) "Dividing elements among cycles in which they participate"
+    !For efficient MPI, PCI-e, packing, and unpacking, we need to separate out the cycles by dependence. Once on cycle has packed, then stage the PCI-e D2H, MPI, PCI-e H2D, & internal unpack
+    !We begin by testing what elements contribute to packing in what cycle's MPI data.
+    do ie = 1,nelemd
+      send_elem_mask(ie,:) = .false.
+      do icycle = 1 , nSendCycles
+        do n = 1 , max_neigh_edges
+          if ( elem(ie)%desc%putmapP(n) >= pSchedule%SendCycle(icycle)%ptrP .and. &
+               elem(ie)%desc%putmapP(n) <= pSchedule%SendCycle(icycle)%ptrP + pSchedule%SendCycle(icycle)%lengthP-1 ) then
+            send_elem_mask(ie,icycle) = .true.
+          endif
+        enddo
+      enddo
+      recv_elem_mask(ie,:) = .false.
+      do icycle = 1 , nRecvCycles
+        do n = 1 , max_neigh_edges
+          if ( elem(ie)%desc%getmapP(n) >= pSchedule%RecvCycle(icycle)%ptrP .and. &
+               elem(ie)%desc%getmapP(n) <= pSchedule%RecvCycle(icycle)%ptrP + pSchedule%RecvCycle(icycle)%lengthP-1 ) then
+            recv_elem_mask(ie,icycle) = .true.
+          endif
+        enddo
+      enddo
+    enddo
+    edgebuf_d = 0.
 
+    elem_computed = .false.   !elem_computed tells us whether an element has been touched by a cycle
+    !This pass accumulates for each cycle incides participating in the MPI_Isend
+    do icycle = 1 , nSendCycles
+      send_nelem(icycle) = 0
+      do ie = 1 , nelemd
+        if ( send_elem_mask(ie,icycle) ) then
+          send_nelem(icycle) = send_nelem(icycle) + 1
+          send_indices(send_nelem(icycle),icycle) = ie
+          elem_computed(ie) = .true.
+        endif
+      enddo
+    enddo
+    total_work = sum(send_nelem)
+    do ie = 1 , nelemd
+      if (.not. elem_computed(ie)) total_work = total_work + 1
+    enddo
+    !This pass adds to each cycle the internal elements not participating in MPI_Isend, so as to even distribute them across cycles.
+    do icycle = 1 , nSendCycles
+      do ie = 1 , nelemd
+        if ( .not. elem_computed(ie) .and. send_nelem(icycle) < int(ceiling(total_work/dble(nSendCycles))) ) then
+          send_nelem(icycle) = send_nelem(icycle) + 1
+          send_indices(send_nelem(icycle),icycle) = ie
+          elem_computed(ie) = .true.
+        endif
+      enddo
+    enddo
+
+    elem_computed = .false.
+    !This pass accumulates for each cycle incides participating in the MPI_Irecv
+    do icycle = 1 , nRecvCycles
+      recv_nelem(icycle) = 0
+      do ie = 1 , nelemd
+        if ( recv_elem_mask(ie,icycle) .and. ( .not. elem_computed(ie) ) ) then
+          recv_nelem(icycle) = recv_nelem(icycle) + 1
+          recv_indices(recv_nelem(icycle),icycle) = ie
+          elem_computed(ie) = .true.
+        endif
+      enddo
+    enddo
+    !This pass accumulates all elements from all cycles participating in MPI_Irecv into the recv_external_indices array
+    recv_external_nelem = 0
+    do icycle = 1 , nRecvCycles
+      do ie = 1 , recv_nelem(icycle)
+        recv_external_nelem = recv_external_nelem + 1
+        recv_external_indices(recv_external_nelem) = recv_indices(ie,icycle)
+      enddo
+    enddo
+    !This pass goes through all elements, and distributes evenly the elements not participating in MPI_Irecv 
+    recv_internal_nelem = 0
+    do ie = 1 , nelemd
+      if ( .not. elem_computed(ie) ) then
+        recv_internal_nelem = recv_internal_nelem + 1
+        recv_internal_indices(recv_internal_nelem) = ie
+      endif
+    enddo
+    !This pass adds to each cycle the internal elements not participating in MPI_Irecv, so as to even distribute them across cycles.
+    do icycle = 1 , nRecvCycles
+      do ie = 1 , nelemd
+        if ( .not. elem_computed(ie) .and. recv_nelem(icycle) < int(ceiling(nelemd/dble(nRecvCycles))) ) then
+          recv_nelem(icycle) = recv_nelem(icycle) + 1
+          recv_indices(recv_nelem(icycle),icycle) = ie
+          elem_computed(ie) = .true.
+        endif
+      enddo
+    enddo
+
+    old_peu = .false.
+    do icycle = 1 , nSendCycles
+      if (send_nelem(icycle) == 0) then
+        write(*,*) 'WARNING: ZERO ELEMENT CYCLES EXIST. A BETTER DECOMPOSITION WILL RUN FASTER IN THE PACK-EXCHANGE-UNPACK.'
+        old_peu = .true.
+      endif
+    enddo
+
+    write(*,*) "Sending element & cycle informationt to device"
+    ierr = cudaMemcpy(send_nelem_d           ,send_nelem           ,size(send_nelem           ),cudaMemcpyHostToDevice); if(ierr.ne.0) stop __LINE__
+    ierr = cudaMemcpy(recv_nelem_d           ,recv_nelem           ,size(recv_nelem           ),cudaMemcpyHostToDevice); if(ierr.ne.0) stop __LINE__
+    ierr = cudaMemcpy(send_indices_d         ,send_indices         ,size(send_indices         ),cudaMemcpyHostToDevice); if(ierr.ne.0) stop __LINE__
+    ierr = cudaMemcpy(recv_indices_d         ,recv_indices         ,size(recv_indices         ),cudaMemcpyHostToDevice); if(ierr.ne.0) stop __LINE__
+    ierr = cudaMemcpy(recv_internal_indices_d,recv_internal_indices,size(recv_internal_indices),cudaMemcpyHostToDevice); if(ierr.ne.0) stop __LINE__
+    ierr = cudaMemcpy(recv_external_indices_d,recv_external_indices,size(recv_external_indices),cudaMemcpyHostToDevice); if(ierr.ne.0) stop __LINE__
+
     write(*,*)"done cuda_mod_init"
 !$OMP END MASTER
 !$OMP BARRIER
   end subroutine cuda_mod_init
 
+
+
+
   !Meaningless kernel just to get initial kernel overheads out of the way.
   attributes(global) subroutine warmup(a)
     integer,value :: a
@@ -94,17 +370,6 @@
 
 
   subroutine euler_step_cuda( np1_qdp , n0_qdp , dt , elem , hvcoord , hybrid , deriv , nets , nete , DSSopt , rhs_multiplier )
-  ! ===================================
-  ! This routine is the basic foward
-  ! euler component used to construct RK SSP methods
-  !
-  !           u(np1) = u(n0) + dt2*DSS[ RHS(u(n0)) ]
-  !
-  ! n0 can be the same as np1.  
-  !
-  ! DSSopt = DSSeta or DSSomega:   also DSS eta_dot_dpdn or omega
-  !
-  ! ===================================
   use kinds             , only: real_kind
   use dimensions_mod    , only: np, npdg, nlev, qsize
   use hybrid_mod        , only: hybrid_t
@@ -140,13 +405,13 @@
   integer :: ie,q,i,j,k
   integer :: rhs_viss = 0
 
-#if (defined ELEMENT_OPENMP)
-write(*,*) 'ERROR: Do not use ELEMENT_OPENMP and CUDA FORTRAN'
-stop
-#endif
-! call t_barrierf('sync_euler_step', hybrid%par%comm)
+  integer :: ierr
+  type(dim3) :: blockdim , griddim
+
   call t_startf('euler_step')
 
+  !This is a departure from the original order, adding an extra MPI communication. It's advantageous because it simplifies
+  !the Pack-Exchange-Unpack procedure for us, since we're adding complexity to overlap MPI and packing
   if ( DSSopt /= DSSno_var ) then
     do ie = nets , nete
       if ( DSSopt == DSSeta         ) DSSvar => elem(ie)%derived%eta_dot_dpdn(:,:,:)
@@ -175,34 +440,6 @@
   !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
   rhs_viss = 0
   if ( limiter_option == 8 .or. nu_p > 0 ) then
-    ! for limiter=0,4 or 8 we will apply dissipation in the RHS,
-    ! when running lim8, we also need to limit the biharmonic, so that term needs
-    ! to be included in each euler step.  three possible algorithms here:
-    ! 1) most expensive:
-    !     compute biharmonic (which also computes qmin/qmax) during all 3 stages
-    !     be sure to set rhs_viss=1
-    !     cost:  3 biharmonic steps with 3 DSS
-    !
-    ! 2) cheapest:
-    !     compute biharmonic (which also computes qmin/qmax) only on first stage
-    !     be sure to set rhs_viss=3
-    !     reuse qmin/qmax for all following stages (but update based on local qmin/qmax)
-    !     cost:  1 biharmonic steps with 1 DSS
-    !     main concern:  viscosity 
-    !     
-    ! 3)  compromise:
-    !     compute biharmonic (which also computes qmin/qmax) only on last stage
-    !     be sure to set rhs_viss=3
-    !     compute qmin/qmax directly on first stage
-    !     reuse qmin/qmax for 2nd stage stage (but update based on local qmin/qmax)
-    !     cost:  1 biharmonic steps, 2 DSS
-    !
-    !  NOTE  when nu_p=0 (no dissipation applied in dynamics to dp equation), we should
-    !        apply dissipation to Q (not Qdp) to preserve Q=1
-    !        i.e.  laplace(Qdp) ~  dp0 laplace(Q)                
-    !        for nu_p=nu_q>0, we need to apply dissipation to Q * diffusion_dp
-    !
-    ! initialize dp, and compute Q from Qdp (and store Q in Qtens_biharmonic)
     do ie = nets , nete
       ! add hyperviscosity to RHS.  apply to Q at timelevel n0, Qdp(n0)/dp
       do k = 1 , nlev    !  Loop index added with implicit inversion (AAM)
@@ -288,7 +525,6 @@
     endif
   endif  ! compute biharmonic mixing term and qmin/qmax
 
-
   !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
   !   2D Advection step
   !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
@@ -334,34 +570,569 @@
       do k = 1 , nlev
         elem(ie)%state%Qdp(:,:,k,q,np1_qdp) = elem(ie)%spheremp(:,:) * Qtens(:,:,k) 
       enddo
-
-      if ( limiter_option == 4 ) then
-        !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!1
-        ! sign-preserving limiter, applied after mass matrix
-        !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!1
-        call limiter2d_zero( elem(ie)%state%Qdp(:,:,:,q,np1_qdp) , hvcoord ) 
-      endif
     enddo
+  enddo
 
-    call edgeVpack(edgeAdv , elem(ie)%state%Qdp(:,:,:,:,np1_qdp) , nlev*qsize , 0 , elem(ie)%desc )
+
+
+  !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+!$OMP BARRIER
+!$OMP MASTER
+  ierr = cudaThreadSynchronize()
+  do ie = 1,nelemd
+    ierr = cudaMemcpy( qdp_d(1,1,1,1,np1_qdp,ie) , elem(ie)%state%qdp(1,1,1,1,np1_qdp) , size(elem(ie)%state%qdp(:,:,:,:,np1_qdp)) , cudaMemcpyHostToDevice )
   enddo
+  ierr = cudaThreadSynchronize()
 
-  call bndry_exchangeV( hybrid , edgeAdv )
+  if ( limiter_option == 4 ) then
+    blockdim = dim3( np, np, nlev )
+    griddim  = dim3( qsize_d, nelemd , 1 )
+    call limiter2d_zero_kernel<<<griddim,blockdim>>>( qdp_d , 1 , nelemd , np1_qdp )
+  endif
+  call pack_exchange_unpack_stage(np1_qdp,hybrid,qdp_d,timelevels)
+  blockdim = dim3( np*np   , nlev   , 1 )
+  griddim  = dim3( qsize_d , nelemd , 1 )
+  call euler_hypervis_kernel_last<<<griddim,blockdim>>>( qdp_d , rspheremp_d , 1 , nelemd , np1_qdp )
 
-  do ie = nets , nete
-    call edgeVunpack( edgeAdv , elem(ie)%state%Qdp(:,:,:,:,np1_qdp) , nlev*qsize , 0 , elem(ie)%desc )
-    do q = 1 , qsize
-      do k = 1 , nlev    !  Potential loop inversion (AAM)
-        elem(ie)%state%Qdp(:,:,k,q,np1_qdp) = elem(ie)%rspheremp(:,:) * elem(ie)%state%Qdp(:,:,k,q,np1_qdp)
-      enddo
-    enddo
+
+  ierr = cudaThreadSynchronize()
+  do ie = 1,nelemd
+    ierr = cudaMemcpy( elem(ie)%state%qdp(1,1,1,1,np1_qdp) , qdp_d(1,1,1,1,np1_qdp,ie) , size(elem(ie)%state%qdp(:,:,:,:,np1_qdp)) , cudaMemcpyDeviceToHost )
   enddo
+  ierr = cudaThreadSynchronize()
+!$OMP END MASTER
+!$OMP BARRIER
+  !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+
   call t_stopf('euler_step')
 end subroutine euler_step_cuda
 
 
 
 
+attributes(global) subroutine limiter2d_zero_kernel(Qdp,nets,nete,np1)
+  use kinds, only : real_kind
+  use dimensions_mod, only : np, nlev
+  implicit none
+  real (kind=real_kind), intent(inout) :: Qdp(np,np,nlev,qsize_d,timelevels,nets:nete)
+  integer, value       , intent(in   ) :: nets,nete,np1
+  integer :: i, j, k, q, ie, jj, tid, ind
+  real (kind=real_kind) :: mass,mass_new
+  real (kind=real_kind), shared :: Qdp_shared((np*np+PAD)*nlev)
+  real (kind=real_kind), shared :: mass_shared(nlev)
+  real (kind=real_kind), shared :: mass_new_shared(nlev)
+
+  i  = threadidx%x
+  j  = threadidx%y
+  k  = threadidx%z
+  q  = blockidx%x
+  ie = blockidx%y
+
+  tid = (threadidx%z-1)*(np*np    ) + (threadidx%y-1)*(np) + threadidx%x
+  ind = (threadidx%z-1)*(np*np+PAD) + (threadidx%y-1)*(np) + threadidx%x
+
+  Qdp_shared(ind) = Qdp(i,j,k,q,np1,ie)
+  call syncthreads()
+
+  if ( tid <= nlev ) then
+    mass = 0.
+    do jj = 1 , np*np
+      mass = mass + Qdp_shared((tid-1)*(np*np+PAD)+jj)
+    enddo
+    mass_shared(tid) = mass
+  endif
+  call syncthreads()
+
+  if ( mass_shared(k)  < 0 ) Qdp_shared(ind) = -Qdp_shared(ind)
+  if ( Qdp_shared(ind) < 0 ) Qdp_shared(ind) = 0
+  call syncthreads()
+
+  if ( tid <= nlev ) then
+    mass = 0.
+    do jj = 1 , np*np
+      mass = mass + Qdp_shared((tid-1)*(np*np+PAD)+jj)
+    enddo
+    mass_new_shared(tid) = mass
+  endif
+  call syncthreads()
+
+  ! now scale the all positive values to restore mass
+  if ( mass_new_shared(k) > 0 ) Qdp_shared(ind) =  Qdp_shared(ind) * abs(mass_shared(k)) / mass_new_shared(k)
+  if ( mass_shared    (k) < 0 ) Qdp_shared(ind) = -Qdp_shared(ind)
+  Qdp(i,j,k,q,np1,ie) = Qdp_shared(ind)
+end subroutine limiter2d_zero_kernel
+
+
+
+
+attributes(global) subroutine euler_hypervis_kernel_last( Qdp , rspheremp , nets , nete , np1 )
+  implicit none
+  real(kind=real_kind), dimension(np*np,nlev,qsize_d,timelevels,nets:nete), intent(inout) :: Qdp
+  real(kind=real_kind), dimension(np*np                        ,nets:nete), intent(in   ) :: rspheremp
+  integer, value                                                          , intent(in   ) :: nets , nete , np1
+  integer :: i, k, q, ie
+  i  = threadidx%x
+  k  = threadidx%y
+  q  = blockidx%x
+  ie = blockidx%y
+  Qdp(i,k,q,np1,ie) = rspheremp(i,ie) * Qdp(i,k,q,np1,ie)
+end subroutine euler_hypervis_kernel_last
+
+
+
+
+subroutine pack_exchange_unpack_stage(np1,hybrid,array_in,tl_in)
+  use hybrid_mod, only : hybrid_t
+  use schedule_mod, only : schedule_t, schedule, cycle_t
+  use parallel_mod, only : abortmp, status, srequest, rrequest, mpireal_t, mpiinteger_t, iam
+  implicit none
+  include 'mpif.h'
+  type(hybrid_t)              , intent(in   ) :: hybrid
+  real(kind=real_kind), device, intent(inout) :: array_in(np,np,nlev,qsize_d,tl_in,nelemd)
+  integer, value              , intent(in   ) :: np1 , tl_in
+  ! local
+  type(dim3)                :: griddim6,blockdim6
+  integer                   :: icycle,nSendCycles,nRecvCycles,n, ierr
+  type (Schedule_t),pointer :: pSchedule
+  type (Cycle_t),pointer    :: pCycle
+  integer                   :: dest,length,tag,iptr,source,nlyr,query_sum, npacked
+  logical :: recvflag, internal_unpacked
+  real :: time_milli
+#ifdef _PREDICT
+   pSchedule => Schedule(iam)
+#else
+   pSchedule => Schedule(1)
+#endif
+   nlyr = edgeAdv%nlyr
+   nSendCycles = pSchedule%nSendCycles
+   nRecvCycles = pSchedule%nRecvCycles
+   d2h_done = .false.
+   msg_sent = .false.
+   h2d_done = .false.
+   internal_unpacked = .false.
+   nmsg_rcvd = 0
+   nmsg_sent = 0
+
+
+  ierr = cudaThreadSynchronize()
+  do icycle=1,nRecvCycles
+    pCycle => pSchedule%RecvCycle(icycle)
+    source  = pCycle%source - 1
+    length  = nlyr * pCycle%lengthP
+    tag     = pCycle%tag
+    call MPI_Irecv(recvbuf_h(1,1,icycle),length,MPIreal_t,source,tag,hybrid%par%comm,Rrequest(icycle),ierr)
+  enddo
+  if (old_peu) then
+    blockdim6 = dim3( np      , np     , nlev )
+    griddim6  = dim3( qsize_d , nelemd , 1    )
+    call edgeVpack_kernel<<<griddim6,blockdim6>>>(edgebuf_d,array_in,putmapP_d,reverse_d,nbuf,0,1,nelemd,np1,tl_in)
+    ierr = cudaThreadSynchronize()
+  else
+    do icycle = 1 , nSendCycles
+      blockdim6 = dim3( np      , np                 , nlev )
+      griddim6  = dim3( qsize_d , send_nelem(icycle) , 1    )
+      call edgeVpack_kernel_stage<<<griddim6,blockdim6,0,streams2(icycle)>>>(edgebuf_d,array_in,putmapP_d,reverse_d,nbuf,0,1,nelemd,np1,send_indices_d,nSendCycles,icycle,tl_in)
+    enddo
+  endif
+  do while ( nmsg_rcvd < nRecvCycles .or. nmsg_sent < nSendCycles .or. .not. internal_unpacked )
+    !When this cycle's D2H memcpy is finished, call the MPI_Isend to shoot it over to the destination process
+    do icycle = 1 , nSendCycles
+      if ( .not. d2h_done(icycle) ) then
+        if ( cudaStreamQuery(streams2(icycle)) == 0 ) then
+          pCycle => pSchedule%SendCycle(icycle)
+          iptr   =  pCycle%ptrP
+          ierr = cudaMemcpyAsync(sendbuf_h(1,1,icycle),edgebuf_d(1,iptr),size(sendbuf_h(1:nlyr,1:pCycle%lengthP,icycle)),cudaMemcpyDeviceToHost,streams(icycle))
+          d2h_done(icycle) = .true.
+        endif
+      endif
+      if ( .not. msg_sent(icycle) ) then  !Only send once per cycle
+        if ( d2h_done(icycle) ) then
+          if ( cudaStreamQuery(streams(icycle)) == 0 ) then
+            pCycle => pSchedule%SendCycle(icycle)
+            dest   =  pCycle%dest - 1
+            iptr   =  pCycle%ptrP
+            length =  nlyr * pCycle%lengthP
+            tag    =  pCycle%tag
+            call MPI_Isend(sendbuf_h(1,1,icycle),length,MPIreal_t,dest,tag,hybrid%par%comm,Srequest(icycle),ierr)
+            msg_sent(icycle) = .true.
+            nmsg_sent = nmsg_sent + 1
+          endif
+        endif
+      endif
+    enddo
+    if (.not. internal_unpacked) then
+      if (nmsg_sent == nSendCycles) then
+        blockdim6 = dim3( np      , np                  , nlev )
+        griddim6  = dim3( qsize_d , recv_internal_nelem , 1    )
+        call edgeVunpack_kernel_stage<<<griddim6,blockdim6>>>(edgebuf_d,array_in,getmapP_d,nbuf,0,1,nelemd,np1,recv_internal_indices_d,tl_in)
+        internal_unpacked = .true.
+      endif
+    endif
+    !When this cycle's MPI transfer is compliete, then call the D2H memcopy asynchronously
+    do icycle = 1 , nRecvCycles
+      if ( .not. h2d_done(icycle) ) then  !Only host to device once per cycle
+        call MPI_Test(Rrequest(icycle),recvflag,status,ierr)
+        if ( (ierr==MPI_SUCCESS) .and. recvflag ) then
+          pCycle => pSchedule%RecvCycle(icycle)
+          iptr   =  pCycle%ptrP
+          ierr = cudaMemcpyAsync(recvbuf_d(1,1,icycle),recvbuf_h(1,1,icycle),size(recvbuf_h(1:nlyr,1:pCycle%lengthP,icycle)),cudaMemcpyHostToDevice,streams(icycle))
+          h2d_done(icycle) = .true.
+          nmsg_rcvd = nmsg_rcvd + 1 !This is how we close the polling loop, once every message has been received
+        endif
+      endif
+    enddo
+  enddo
+  call MPI_WaitAll(nSendCycles,Srequest,status,ierr)
+  do icycle = 1 , nRecvCycles
+    pCycle => pSchedule%RecvCycle(icycle)
+    iptr   =  pCycle%ptrP
+    ierr = cudaMemcpyAsync(edgebuf_d(1,iptr),recvbuf_d(1,1,icycle),size(recvbuf_h(1:nlyr,1:pCycle%lengthP,icycle)),cudaMemcpyDeviceToDevice,streams(icycle))
+  enddo
+  ierr = cudaThreadSynchronize()
+  blockdim6 = dim3( np      , np                  , nlev )
+  griddim6  = dim3( qsize_d , recv_external_nelem , 1    )
+  call edgeVunpack_kernel_stage<<<griddim6,blockdim6>>>(edgebuf_d,array_in,getmapP_d,nbuf,0,1,nelemd,np1,recv_external_indices_d,tl_in)
+  ierr = cudaThreadSynchronize()
+
+
+! ierr = cudaThreadSynchronize()
+! do icycle=1,nRecvCycles
+!   pCycle => pSchedule%RecvCycle(icycle)
+!   source  = pCycle%source - 1
+!   length  = nlyr * pCycle%lengthP
+!   tag     = pCycle%tag
+!   call MPI_Irecv(recvbuf_h(1,1,icycle),length,MPIreal_t,source,tag,hybrid%par%comm,Rrequest(icycle),ierr)
+! enddo
+! blockdim6 = dim3( np      , np     , nlev )
+! griddim6  = dim3( qsize_d , nelemd , 1    )
+! call edgeVpack_kernel<<<griddim6,blockdim6>>>(edgebuf_d,array_in,putmapP_d,reverse_d,nbuf,0,1,nelemd,np1,tl_in)
+! ierr = cudaThreadSynchronize()
+! do icycle = 1 , nSendCycles
+!   pCycle => pSchedule%SendCycle(icycle)
+!   iptr   =  pCycle%ptrP
+!   ierr = cudaMemcpyAsync(sendbuf_h(1,1,icycle),edgebuf_d(1,iptr),size(sendbuf_h(1:nlyr,1:pCycle%lengthP,icycle)),cudaMemcpyDeviceToHost,streams(icycle))
+! enddo
+! ierr = cudaThreadSynchronize()
+! do icycle = 1 , nSendCycles
+!   pCycle => pSchedule%SendCycle(icycle)
+!   dest   =  pCycle%dest - 1
+!   iptr   =  pCycle%ptrP
+!   length =  nlyr * pCycle%lengthP
+!   tag    =  pCycle%tag
+!   call MPI_Isend(sendbuf_h(1,1,icycle),length,MPIreal_t,dest,tag,hybrid%par%comm,Srequest(icycle),ierr)
+! enddo
+! call MPI_WaitAll(nRecvCycles,Rrequest,status,ierr)
+! call MPI_WaitAll(nSendCycles,Srequest,status,ierr)
+! !When this cycle's MPI transfer is compliete, then call the D2H memcopy asynchronously
+! do icycle = 1 , nRecvCycles
+!   pCycle => pSchedule%RecvCycle(icycle)
+!   iptr   =  pCycle%ptrP
+!   ierr = cudaMemcpyAsync(edgebuf_d(1,iptr),recvbuf_h(1,1,icycle),size(recvbuf_h(1:nlyr,1:pCycle%lengthP,icycle)),cudaMemcpyHostToDevice,streams(icycle))
+! enddo
+! blockdim6 = dim3( np      , np     , nlev )
+! griddim6  = dim3( qsize_d , nelemd , 1    )
+! ierr = cudaThreadSynchronize()
+! call edgeVunpack_kernel<<<griddim6,blockdim6>>>(edgebuf_d,array_in,getmapP_d,nbuf,0,1,nelemd,np1,tl_in)
+! ierr = cudaThreadSynchronize()
+
+end subroutine pack_exchange_unpack_stage
+
+
+
+
+attributes(global) subroutine edgeVpack_kernel_stage(edgebuf,v,putmapP,reverse,nbuf,kptr,nets,nete,nt,send_indices,nSendCycles,icycle,tl_in)
+  use control_mod, only : north, south, east, west, neast, nwest, seast, swest
+  implicit none
+  real (kind=real_kind), intent(  out) :: edgebuf(nlev*qsize_d,nbuf)
+  integer              , intent(in   ) :: putmapP(max_neigh_edges,nets:nete)
+  logical              , intent(in   ) :: reverse(max_neigh_edges,nets:nete)
+  real (kind=real_kind), intent(in   ) :: v(np*np,nlev,qsize_d,tl_in,nets:nete)
+  integer              , intent(in   ) :: send_indices(nets:nete,nSendCycles)
+  integer, value       , intent(in   ) :: kptr,nets,nete,nt,nbuf,nSendCycles,icycle,tl_in
+  integer :: i,j,k,q,l,offset,ij,ijk,ti,tj,tk,x,y,ir,  reverse_south, reverse_north, reverse_west, reverse_east, el
+  integer, shared :: ic(max_corner_elem,4), direction(4), reverse_direction(4)
+  real (kind=real_kind), shared :: vshrd(nlev+PAD,np,np)
+  i  = threadidx%x
+  j  = threadidx%y
+  k  = threadidx%z
+  q  = blockidx%x
+  el = send_indices(blockidx%y,icycle)
+  
+  ij = (j-1)*np+i
+  ijk = (k-1)*np*np + (j-1)*np + i -1
+  tk = mod( ijk, nlev ) + 1
+  ti = mod( ijk/nlev, np ) + 1
+  tj = ijk/(nlev*np) + 1
+
+  if( i+j+k == blockdim%x + blockdim%y + blockdim%z ) then
+    direction(west_px)  = putmapP(west ,el)
+    direction(east_px)  = putmapP(east ,el)
+    direction(south_px) = putmapP(south,el)
+    direction(north_px) = putmapP(north,el)
+    reverse_direction(south_px) = reverse(south,el)
+    reverse_direction(north_px) = reverse(north,el)
+    reverse_direction(west_px)  = reverse(west,el)
+    reverse_direction(east_px)  = reverse(east,el)
+  endif
+  if( ijk < max_corner_elem ) then
+    ic(ijk+1,1) = putmapP(swest+ijk,el)+1
+    ic(ijk+1,2) = putmapP(seast+ijk,el)+1
+    ic(ijk+1,3) = putmapP(nwest+ijk,el)+1
+    ic(ijk+1,4) = putmapP(neast+ijk,el)+1
+  endif
+  vshrd(k,i,j) = v(ij,k,q,nt,el)
+  
+  call syncthreads()
+   
+  offset = (q-1)*nlev + tk + kptr
+  ir = np-ti+1
+  if( 1==tj .or. 4==tj ) then
+    if( reverse_direction(tj) ) then; edgebuf(offset,direction(tj)+ti) = vshrd(tk,ir,tj)
+    else                            ; edgebuf(offset,direction(tj)+ti) = vshrd(tk,ti,tj); endif
+  endif
+  if( 2==tj ) then
+    if( reverse_direction(2) ) then; edgebuf(offset,direction(tj)+ti) = vshrd(tk,1,ir)
+    else                           ; edgebuf(offset,direction(tj)+ti) = vshrd(tk,1,ti); endif
+  endif
+  if( 3==tj ) then
+    if( reverse_direction(3) ) then; edgebuf(offset,direction(tj)+ti) = vshrd(tk,4,ir)
+    else                           ; edgebuf(offset,direction(tj)+ti) = vshrd(tk,4,ti); endif
+  endif
+  if( tj==1 ) then
+    do l=1, max_corner_elem       
+      x = mod(ti-1,2)*(np-1) + 1  ! we need to convert ti index from {1,2,3,4} to {(1,1),(4,1),(1,4),(4,4)}
+      y = ((ti-1)/2)*(np-1) + 1   !   so, ti->(x,y)
+      if( ic(l,ti) /= 0 ) edgebuf( offset, ic(l,ti) ) = vshrd(tk,x,y)
+    enddo
+  endif
+end subroutine edgeVpack_kernel_stage
+
+
+
+attributes(global) subroutine edgeVunpack_kernel_stage(edgebuf,v,getmapP,nbuf,kptr,nets,nete,nt,recv_indices,tl_in)
+  use control_mod, only : north, south, east, west, neast, nwest, seast, swest
+  implicit none
+  real (kind=real_kind), intent(in   ) :: edgebuf(nlev*qsize_d,nbuf)
+  integer              , intent(in   ) :: getmapP(max_neigh_edges,nets:nete)
+  real (kind=real_kind), intent(inout) :: v(np*np,nlev,qsize_d,tl_in,nets:nete)
+  integer              , intent(in   ) :: recv_indices(nets:nete)
+  integer, value       , intent(in   ) :: kptr,nets,nete,nt,nbuf,tl_in
+  integer :: i,j,k,l,q,el,offset,ij,ti,tj,tk,ijk,x,y,tij
+  integer, shared :: direction(4),is,ie,in,iw, ic(max_corner_elem,4)
+  real (kind=real_kind), shared :: vshrd(nlev+PAD,np,np)
+  real (kind=real_kind) :: v_before_update, neighbor_value
+  
+  i  = threadidx%x
+  j  = threadidx%y
+  k  = threadidx%z
+  q  = blockidx%x
+  el = recv_indices(blockidx%y)
+  
+  ij = (j-1)*np+i
+  ijk = (k-1)*np*np + ij -1
+  tk = mod( ijk, nlev ) + 1
+  ti = mod( ijk/nlev, np ) + 1
+  tj = ijk/(nlev*np) + 1
+  
+  if( i + j + k == np+np+nlev ) then
+    direction(west_px)  = getmapP(west ,el)
+    direction(east_px)  = getmapP(east ,el)
+    direction(south_px) = getmapP(south,el)
+    direction(north_px) = getmapP(north,el)
+  endif
+  if( ijk < max_corner_elem) then
+    ic(ijk+1,1) = getmapP(swest+ijk,el)+1
+    ic(ijk+1,2) = getmapP(seast+ijk,el)+1
+    ic(ijk+1,3) = getmapP(nwest+ijk,el)+1
+    ic(ijk+1,4) = getmapP(neast+ijk,el)+1
+  endif
+  vshrd(k,i,j) = 0.D0
+  call syncthreads()
+    
+  offset = (q-1)*nlev + tk + kptr
+  neighbor_value = edgebuf( offset, direction(tj)+ti ) ! load neighbor values into registers 
+                                                       !  nlev x np consecutive threads contain all the face values
+                                                       !   tj = 1:  south   
+                                                       !   tj = 2:  west
+                                                       !   tj = 3:  east
+                                                       !   tj = 4:  north
+                                                       
+  ! combine the neighbor values in smem
+  if( 1==tj .or. 4==tj ) vshrd(tk, ti, tj) = neighbor_value  ! add the south and north values to smem
+  call syncthreads() ! this sync is needed to avoid race conditions (east/west share corners with sourth/north)
+  if( 2==tj ) vshrd(tk,1,ti) = vshrd(tk,1,ti) + neighbor_value  ! update west
+  if( 3==tj ) vshrd(tk,4,ti) = vshrd(tk,4,ti) + neighbor_value  ! update east
+  call syncthreads()
+
+  v_before_update = v(ij,k,q,nt,el) ! start loading the local value to be updated with neibhbor values
+  
+  ! update the "corner" columns
+  if( tj==1 ) then
+    do l=1, max_corner_elem       
+      x = mod(ti-1,2)*(np-1) + 1  ! we need to convert ti index from {1,2,3,4} to {(1,1),(4,1),(1,4),(4,4)}
+      y = ((ti-1)/2)*(np-1) + 1   !   so, ti->(x,y)
+      if( ic(l,ti) /= 0 ) vshrd(tk,x,y) = vshrd(tk,x,y) + edgebuf( offset, ic(l,ti) )
+    enddo
+  endif
+  call syncthreads()
+    
+  v(ij,k,q,nt,el) = v_before_update + vshrd(k,i,j)
+end subroutine edgeVunpack_kernel_stage
+
+
+
+
+attributes(global) subroutine edgeVpack_kernel(edgebuf,v,putmapP,reverse,nbuf,kptr,nets,nete,nt,tl_in)
+  use control_mod, only : north, south, east, west, neast, nwest, seast, swest
+  implicit none
+  real (kind=real_kind), intent(  out) :: edgebuf(nlev*qsize_d,nbuf)
+  integer              , intent(in   ) :: putmapP(max_neigh_edges,nets:nete)
+  logical              , intent(in   ) :: reverse(max_neigh_edges,nets:nete)
+  real (kind=real_kind), intent(in   ) :: v(np*np,nlev,qsize_d,tl_in,nets:nete)
+  integer, value       , intent(in   ) :: kptr,nets,nete,nt,nbuf,tl_in
+  integer :: i,j,k,q,l,offset,ij,ijk,ti,tj,tk,x,y,ir,  reverse_south, reverse_north, reverse_west, reverse_east, el
+  integer, shared :: ic(max_corner_elem,4), direction(4), reverse_direction(4)
+  real (kind=real_kind), shared :: vshrd(nlev+PAD,np,np)
+  i  = threadidx%x
+  j  = threadidx%y
+  k  = threadidx%z
+  q  = blockidx%x
+  el = blockidx%y
+  
+  ij = (j-1)*np+i
+  ijk = (k-1)*np*np + (j-1)*np + i -1
+  tk = mod( ijk, nlev ) + 1
+  ti = mod( ijk/nlev, np ) + 1
+  tj = ijk/(nlev*np) + 1
+
+  if( i+j+k == blockdim%x + blockdim%y + blockdim%z ) then
+    direction(west_px)  = putmapP(west ,el)
+    direction(east_px)  = putmapP(east ,el)
+    direction(south_px) = putmapP(south,el)
+    direction(north_px) = putmapP(north,el)
+    reverse_direction(south_px) = reverse(south,el)
+    reverse_direction(north_px) = reverse(north,el)
+    reverse_direction(west_px)  = reverse(west,el)
+    reverse_direction(east_px)  = reverse(east,el)
+  endif
+  if( ijk < max_corner_elem ) then
+    ic(ijk+1,1) = putmapP(swest+ijk,el)+1
+    ic(ijk+1,2) = putmapP(seast+ijk,el)+1
+    ic(ijk+1,3) = putmapP(nwest+ijk,el)+1
+    ic(ijk+1,4) = putmapP(neast+ijk,el)+1
+  endif
+  vshrd(k,i,j) = v(ij,k,q,nt,el)
+  
+  call syncthreads()
+   
+  offset = (q-1)*nlev + tk + kptr
+  ir = np-ti+1
+  if( 1==tj .or. 4==tj ) then
+    if( reverse_direction(tj) ) then; edgebuf(offset,direction(tj)+ti) = vshrd(tk,ir,tj)
+    else                            ; edgebuf(offset,direction(tj)+ti) = vshrd(tk,ti,tj); endif
+  endif
+  if( 2==tj ) then
+    if( reverse_direction(2) ) then; edgebuf(offset,direction(tj)+ti) = vshrd(tk,1,ir)
+    else                           ; edgebuf(offset,direction(tj)+ti) = vshrd(tk,1,ti); endif
+  endif
+  if( 3==tj ) then
+    if( reverse_direction(3) ) then; edgebuf(offset,direction(tj)+ti) = vshrd(tk,4,ir)
+    else                           ; edgebuf(offset,direction(tj)+ti) = vshrd(tk,4,ti); endif
+  endif
+  if( tj==1 ) then
+    do l=1, max_corner_elem       
+      x = mod(ti-1,2)*(np-1) + 1  ! we need to convert ti index from {1,2,3,4} to {(1,1),(4,1),(1,4),(4,4)}
+      y = ((ti-1)/2)*(np-1) + 1   !   so, ti->(x,y)
+      if( ic(l,ti) /= 0 ) edgebuf( offset, ic(l,ti) ) = vshrd(tk,x,y)
+    enddo
+  endif
+end subroutine edgeVpack_kernel
+
+
+
+
+
+attributes(global) subroutine edgeVunpack_kernel(edgebuf,v,getmapP,nbuf,kptr,nets,nete,nt,tl_in)
+  use control_mod, only : north, south, east, west, neast, nwest, seast, swest
+  real (kind=real_kind), intent(in   ) :: edgebuf(nlev*qsize_d,nbuf)
+  integer              , intent(in   ) :: getmapP(max_neigh_edges,nets:nete)
+  real (kind=real_kind), intent(inout) :: v(np*np,nlev,qsize_d,tl_in,nets:nete)
+  integer, value       , intent(in   ) :: kptr,nets,nete,nt,nbuf,tl_in
+  integer :: i,j,k,l,q,el,offset,ij,ti,tj,tk,ijk,x,y,tij
+  integer, shared :: direction(4),is,ie,in,iw, ic(max_corner_elem,4)
+  real (kind=real_kind), shared :: vshrd(nlev+PAD,np,np)
+  real (kind=real_kind) :: v_before_update, neighbor_value
+  
+  i  = threadidx%x
+  j  = threadidx%y
+  k  = threadidx%z
+  q  = blockidx%x
+  el = blockidx%y
+  
+  ij = (j-1)*np+i
+  ijk = (k-1)*np*np + ij -1
+  tk = mod( ijk, nlev ) + 1
+  ti = mod( ijk/nlev, np ) + 1
+  tj = ijk/(nlev*np) + 1
+  
+  if( i + j + k == np+np+nlev ) then
+    direction(west_px)  = getmapP(west ,el)
+    direction(east_px)  = getmapP(east ,el)
+    direction(south_px) = getmapP(south,el)
+    direction(north_px) = getmapP(north,el)
+  endif
+  if( ijk < max_corner_elem) then
+    ic(ijk+1,1) = getmapP(swest+ijk,el)+1
+    ic(ijk+1,2) = getmapP(seast+ijk,el)+1
+    ic(ijk+1,3) = getmapP(nwest+ijk,el)+1
+    ic(ijk+1,4) = getmapP(neast+ijk,el)+1
+  endif
+  vshrd(k,i,j) = 0.D0
+  call syncthreads()
+    
+  offset = (q-1)*nlev + tk + kptr
+  neighbor_value = edgebuf( offset, direction(tj)+ti ) ! load neighbor values into registers 
+                                                       !  nlev x np consecutive threads contain all the face values
+                                                       !   tj = 1:  south   
+                                                       !   tj = 2:  west
+                                                       !   tj = 3:  east
+                                                       !   tj = 4:  north
+                                                       
+  ! combine the neighbor values in smem
+  if( 1==tj .or. 4==tj ) vshrd(tk, ti, tj) = neighbor_value  ! add the south and north values to smem
+  call syncthreads() ! this sync is needed to avoid race conditions (east/west share corners with sourth/north)
+  if( 2==tj ) vshrd(tk,1,ti) = vshrd(tk,1,ti) + neighbor_value  ! update west
+  if( 3==tj ) vshrd(tk,4,ti) = vshrd(tk,4,ti) + neighbor_value  ! update east
+  call syncthreads()
+
+  v_before_update = v(ij,k,q,nt,el) ! start loading the local value to be updated with neibhbor values
+  
+  ! update the "corner" columns
+  if( tj==1 ) then
+    do l=1, max_corner_elem       
+      x = mod(ti-1,2)*(np-1) + 1  ! we need to convert ti index from {1,2,3,4} to {(1,1),(4,1),(1,4),(4,4)}
+      y = ((ti-1)/2)*(np-1) + 1   !   so, ti->(x,y)
+      if( ic(l,ti) /= 0 ) vshrd(tk,x,y) = vshrd(tk,x,y) + edgebuf( offset, ic(l,ti) )
+    enddo
+  endif
+  call syncthreads()
+    
+  v(ij,k,q,nt,el) = v_before_update + vshrd(k,i,j)
+end subroutine edgeVunpack_kernel
+
+
+
+
+
+
+
+
+
+
+
+
+
+
 subroutine limiter2d_zero(Q,hvcoord)
   ! mass conserving zero limiter (2D only).  to be called just before DSS
   !
Index: prim_driver_mod.F90
===================================================================
--- prim_driver_mod.F90	(revision 2389)
+++ prim_driver_mod.F90	(revision 2419)
@@ -1015,7 +1015,7 @@
 
 #ifdef _ACCEL
   !Inside this routine, we enforce an OMP BARRIER and an OMP MASTER. It's left out of here because it's ugly
-  call cuda_mod_init()
+  call cuda_mod_init(elem)
 #endif
 
     call prim_printstate(elem, tl, hybrid,hvcoord,nets,nete, fvm)
